{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GRJKOLTtH0sm",
        "outputId": "f2a32015-d42f-48ad-d32c-59a0ed5e625f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install library\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "r_T9b2DmIjrD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/@souvik_real/exploring-stable-diffusion-for-image-generation-generating-code-in-python-7c1e56371b78\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWsB6T-JT0WB",
        "outputId": "1eaf64d3-db40-422f-ac7e-d5933060e8a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Souvik Das Follow -- Listen Share Stable Diffusion is a cutting-edge deep learning model designed for generating high-quality images based on text prompts. Developed by the team at CompVis, this model leverages the power of diffusion probabilistic models to create detailed and realistic visuals. In this article, we will explore how to use the Stable Diffusion model with Streamlit to build an interactive image generator application. Let’s dive into the code step by step to understand how to set up and use Stable Diffusion for generating images. First, we need to import the necessary libraries: Next, we define a function to load the Stable Diffusion model: The load_model function uses a spinner to indicate loading progress and returns the initialized model pipeline. We set up the Streamlit application with a title and check if the model is already loaded in the session state: If the model is not already loaded, it is loaded and stored in st.session_state to avoid reloading it on every interaction. We create an input field for the user to enter a text prompt and a button to trigger image generation: The final Output came for me : The integration of Stable Diffusion with Streamlit provides an intuitive and interactive way to generate high-quality images from text prompts. By following the steps outlined in this article, you can set up your own image generator and experiment with different prompts to create stunning visuals. This example demonstrates the power of modern AI models and how they can be seamlessly integrated into web applications for practical use. Follow Me https://www.linkedin.com/in/souvik2710/ -- -- Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, Souvik Das looks at how to use the Stable Diffusion model for generating images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/@ignacio.de.gregorio.noblejas/entropy-finally-a-real-cure-to-hallucinations-52d63c23c2f5\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wBAI1P_XqOc",
        "outputId": "7df7fbef-c376-4e81-ae69-93e929365f5a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Member-only story Ignacio de Gregorio Follow -- Share A mysterious researcher (yes, we don’t know who he/she is) has published something that is nothing short of incredible. It has presented highly promising data on a new decoding method (how the large language models, or LLMs, generate the data) called Entropix, which drastically reduces hallucinations, Large Language Models (LLMs) biggest enemy, by leveraging an LLM feature that was hidden in plain sight all along: Uncertainty modeling. In the process, we might have just realized that our methods to minimize hallucinations were, in fact, causing the opposite effect: more hallucinations. This news is so recent that we don’t even have a paper. Still, the AI industry is going wild for it, with some people calling it “the biggest deal since the attention paper,” the research piece that changed everything and is today’s key operator behind all foundation models. As you may assume, that’s quite the statement. But why all this excitement? This article is an extract from my newsletter, the place where AI analysts, strategists, and decision-makers use to find answers to the most pressing questions in AI. -- -- I break down AI in easy-to-understand language for you. Sign up here: https://thetechoasis.beehiiv.com/subscribe Business inquiries: nacho@thewhitebox.ai Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, I break down AI in easy-to-understand language for you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/@francescofranco_39234/convolutional-neural-networks-for-computer-vision-a913e77c60ff\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJHhPtOkYVC8",
        "outputId": "2b1e27de-1aeb-4677-95dc-e70c97e30ef4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Francesco Franco Follow -- Listen Share Machine learning (and consequently deep learning) can be used to train computers to see things. We know that machine learning is about feeding examples to machines, after which they derive the patterns in these examples themselves. Consequently, we can see that using machine learning for computer vision equals showing machines enough examples so that they can learn to recognize them on their own, for new data. In deep learning, we use deep neural networks to teach machines to recognize patterns on their own. But not every class of deep learning algorithms is suitable for computer vision, for reasons that will be explained later in this blog. Nevertheless, there exists a class of networks that can handle this kind of data. Convolutional Neural Networks (CNNs) are a class of Artificial Neural Networks (ANNs) which have proven to be very effective for this type of task. They have certain characteristics that share resemblance with how human beings recognize patterns in visual imagery. But CNN is not one thing. It is a class of algorithms. And it contains various so-called network architectures. Then what is an architecture? Most simply, we can compare an architecture with a building. It consists of walls, windows, doors, et cetera — and together these form the building. Explaining what a neural network architecture is benefits from this analogy. Put simply, it is a collection of components that is put in a particular order. The components themselves may be repeated and also may form blocks of components. Together, these components form a neural network: in this case, a CNN to be precise. In this blog, I would like to explain the generic concepts behind CNNs in more detail. I will cover the differences between regular neural networks and convolutional ones. I will decompose a general neural network into its components — i.e., its layers — and I will give some suggestions about how to construct such a network from scratch. CNNs are quite similar to ‘regular’ neural networks: they are networks of neurons, which receive input, transform the input using mathematical transformations and preferably a non-linear activation function, and they often end in the form of a classifier/regressor. But they are different in the sense that they assume that the input is an image. What’s more, the input is expected to be preprocessed to a minimum extent. Based on this assumption, using various types of layers, we can create architectures which are optimized for computer vision. In order to build our knowledge, we must take one small step back before we can continue. We must recall that a regular neural network consists of a chain of interconnected layers of neurons. One such network may look as follows: As you may recall from my previous post on what deep learning is, such a neural network consists of layers: in its simplest form one input layer, one or multiple hidden layers, and one output layer. The neurons are structured vertically and are interconnected. This means that the output from one neuron in the input layers goes to every neuron in the subsequent layer. This process happens for every layer, as you can see in the model above. This kind of information propagation is highly inefficient when it comes to computer vision. Let’s imagine how you would recognize objects in an image: by taking a close look at the object itself, and possibly its direct surroundings. If you must recognize trees in a photo, you will not take a look at the blue skies at the very top of the image. This is however what would happen if you fed images to a normal ANN: it would take into account the entire image for the computer vision task you want it to perform. This is highly inefficient: both in terms of pragmatic quality (i.e., why the heck do you want to look at the entire image to identify an object within the image?) as well as neural network quality (for large images, the number of parameters skyrockets — and this is not a good thing, for it would greatly increase the odds that overfitting happens). Convolutional neural networks to the rescue. They are specifically designed to be used in computer vision tasks, which means that their design is optimized for processing images. In CNNs, the layers are three dimensional. This means that the neurons are structured in shape of form (width, height, depth). If we have a 50x50 pixels image encoded as RGB (red — green — blue), the shape of the input layer will be (50, 50, 3). You may now ask: why is this better than a regular ANN? Here’s why: we can now use so-called convolutional layers, which allow us to inspect small pieces of the image and draw conclusions for every piece and then merge them together. That’s completely different than these general ANNs, which look at the image as a whole. I will next cover the components of a CNN, including these convolutional layers! Convolutional neural networks share the characteristics of multilayer perceptrons (and may be said to be composed of individual MLPs, although this analogy remains a bit vague): they have one input layer, one output layer and a set of — at minimum one — hidden layer(s) in between. As I wrote before, CNNs are composed of various ‘components’. A component equals at least one layer in the CNN. Now, what are these components? Here they are, in short: However, in order to understand a CNN in a better way, I will first have to look at another term in more detail: the receptive field of a particular layer. I will then continue with the actual layers. Suppose that we have a neural network. In this case, I’ll use a simple one, for clarity, and it is illustrated below. The yellow layer with two neurons is the input layer. It receives the input data. The red layer at the end is the output layer, which provides the output as specified by the architecture. The neural network below consists of one blue layer, a so-called hidden layer. It may be more than possible (and this is the case more often than not) that a model consists of multiple hidden layers. But once again, for clarity, I attempted to keep the model as simple as possible. Every hidden layer is connected to two layers: one from which it receives a signal and one to which it passes the transformed signal. The input layer is connected to a subsequent layer and the output layer is connected to a prior layer. Every layer has a so-called receptive field. This is the relative number (or the percentage, or the ratio, as you desire) of neurons from which it receives a signal. The hidden layer in the network below receives the signal from 2 out of 2 neurons, so its receptive field is the entire previous layer. The same goes for the output layer, which is 5 out of 5. You will see that in CNNs, not every neuron in a layer has a full receptive field. That’s what I meant when I wrote that layers tailored to a CNN will allow us to investigate pieces of images, rather than entire images at once. Let’s proceed with the so-called convolutional layers, in which this principle becomes perfectly clear! In a CNN, the first layer that comes after the input layer is a so-called convolutional layer. In another brilliant piece on CNNs, the analogy to a flashlight was drawn, when explaining what a convolutional layer does. Let’s attempt to recreate this analogy here to make clear how such a layer operates. We have the input layer on the left. This layer contains the actual image — a matrix/tensor/whatever you wish to call it of shape (width, height, depth). In our case, since we used a 50x50 RGB image above, our shape is (50, 50, 3). Note that this means that behind the layer you see in the left of the image, lie two more layers (given the depth of 3). The smaller layer on the right is the first convolutional layer. What it does can be explained using the flashlight analogy. Every neuron has a certain receptive field on the previous layer, the input layer in our case. The receptive field in our example is 5x5x3 pixels (note that we have three layers!). Consequently, the convolutional layer must also have a depth of 3. What we can do now is take a look at a small part of the image and see what’s in it. That’s what I meant when I wrote about it before! In deep learning terms, we also call this neuron a filter. A convolutional layer thus consists of a set of filters which all look at different parts of the image. In our case, the filters look at only 5x5x3 = 75 pixels each, instead of the 50x50x3 = 7500 pixels within the three RGB layers. I then had the following question: what does this filter look like? How do I pick a filter when I start training a CNN? It was extremely unclear to me: it seemed like the models applied filters that could detect various things within an image (e.g., a vertical line), but why didn’t I find about how to pick a filter type when starting training? One has to start somewhere, right? An answer on StackOverflow provided the solution to my confusion. It goes like this when training a CNN: first, for every convolutional layer, the filter is initialized in some way. This may be done randomly, but different initialization methods for CNNs exist. It then trains once and calculates the so-called ‘loss value’, i.e. the difference between the real world and what the model predicted. Based on this loss value, the filter is changed a tiny bit, after which the same thing starts again. This way, the CNN learns to create its filter organically based on the data, while maximizing the performance of the network. Now that we know how a convolutional layer that uses filters to look at smaller areas of the image, we can move on and take a look inside the filter. A neuron produces just one single output value. This means that the 5x5x3 = 75 pixels must be transformed into just one number. But how is this done? Let’s take a look inside the filter to find out more. And especially how it calculates the singular value that the neuron in the convolutional network produces as output. It does so by what we call element wise multiplications. Suppose that we have a picture of a house. Suppose that our network has learned that it must detect vertical lines. What I mean is: it has learned a filter that knows how to detect vertical lines. This filter looks like this: Normally, a filter moves over the image, in order to capture it entirely. We’ll cover that next. But for simplicity, we have just put the filter at a convenient place, exactly where we have a vertical line in our image. Like this: If we zoom in at the image, and especially the part that is covered by the filter, we see this: If we translate this into what a computer sees, remember that it’s a 2D array of pixel intensities. It would thus look something like this internally: The same is true for the filter: Obviously, the model does not learn a filter based on one picture. This means that the pixel intensities for the vertical line filter will probably not exactly match the ones for the image. Additionally, they may be different for different sub parts of the vertical line, as you can see in the computerized version of the filter above. But that does not matter really, since it will still be able to detect a vertical line. And here’s why: What happens next is that the model performs element wise multiplications using the filters that it’s learned, as I wrote before. That’s very simple: if we put the two 2D arrays (also known as matrices) next to each other, we take the two elements at position (i,j) and multiply them. We do this for all the positions in the matrices, and add up the numbers. Visually, we can represent this as the two arrows that iterate over the matrices. The first calculation will be (0 x 0) = 0. We move the arrow one position to the right. The second, third, fourth, up to the eighth, is also 0. But the 9th is (35 x 25) = 875. We remember 875 and move the arrow one step to the right. We continue this process until we have looked at every individual element, and its corresponding variant in the other matrix. We will now have a large set of numbers. Part of the element-wise multiplications used in Convolutional Neural Networks is that in the end we sum them all together, like this: The result is a large number, obviously. It is the output of the neuron of the convolutional layer which looks at its very own receptive field. But what is the significance of this number? That’s very simple: the larger it is, the more the receptive field (i.e., the part of the image) looked at by the neuron matches the filter used (in our case, that the part of the house looked at contains a vertical line). This means that we can now distinguish certain characteristics in parts of an image. But why is that the case? Let’s move the filter a bit, once again to an arbitrary position in the image, to show what I mean. As you may guess, the vertical line filter itself did not change, because we’re using the trained model and thus its learned filters. The computerized version is thus still the same: But the computerized version of the part of the image did change, simply because we let the filter look at another part of the image. What do you think it looks like? If you think it’s a matrix of zeroes, you’re right :-) The result of the formula is now very simple. All the elements are multiplied by 0, and thus the sum will be 0. You can now make a guess how the CNN detects certain patterns in data. If a part of the image looks like something available in a filter, it will produce a rather large number. If the part of the image does not look like the filter at all, the result of the element-wise multiplication will be zero or a very small number! For the examples above, we twice placed our filter on top of an arbitrary part of the image. As you may guess, this is not how Convolutional Neural Networks do really attempt to detect learnt patterns in an image. The process is actually pretty simple, though, and here’s how it is done. The filter starts at the very top left part of the image, for which the element-wise multiplications are computed. It will then move slightly to the right. If it reaches the border, it will move slightly to the bottom and will start again at the very left end of the image. The number of pixels it will move to the right (or towards the bottom whenever necessary) is what we call stride. The higher the stride, the faster it moves over an image, since it will move by more pixels. This means that the model will be smaller and easier to train. But increased speed comes with one downside: the higher the stride, the less detailed it can capture patterns in the image, since it simply does not look at all of its individual parts. Overall, Deep Learning engineers often choose a stride of 1, and sometimes 2. We often favor longer training times with more accurate models over less accurate ones while training time is short. So what we know now is that in a convolutional layer, for new data, the learned filters slide over the image, calculate whether their content is present in the part of the image, and output a number for every step. Once the filter has looked at the entire image, it will be able to draw some kind of a heatmap over the image. The colors of the heatmap will be brightest where the filter was able to detect the pattern that it is trained for. They will be the lightest where, in our case, no vertical lines could be found. For our house, this means that the two vertical lines that represent the walls will most likely light up, while the rest remains darker. We call this heatmap-like structure the activation map and it provides highlights where certain aspects can be found. This activation map will reduce the dimensionality of the input to 1D. Suppose that we have a 32 x 32 RGB image and thus a 3D input of 32x32x3. If we use 5x5 pixel filters, it can take 28 locations horizontally, and 28 vertically. As the filter will look and merge the 3 dimensions, we will end up with an array of shape (28, 28, 1), where 1 stands for the number of filters used. In our case, we simply used one filter, but obviously, multiple ones are used. Being able to detect vertical lines will not be enough to distinguish a house from a lamp post. That’s why more filters are often used. If five are used, the output shape would be (28, 28, 5). From my blog about what deep learning is we learned that neural networks make representations more abstract when one moves through the network and moves to the right. In convolutional neural networks, this is also the case. But why? Quite simple, actually. Suppose that you train a model so that it can distinguish the lamp posts we used in our previous example. It will need to provide as an output “contains lamp post” or “contains no lamp post”. It will thus need to learn filters that can together recognize these lamp posts, of which one obviously will be a vertical line detector. We would need to use various images that contain lamp posts. We feed them to the model during training so that it can learn these particular filters. Obviously, every lamp post is different. No lamp post looks the same because, simply put, every picture is different. This means that when processing the hundreds or thousands of lamp posts it sees in the training data, it would need to generalize a bit, or it will not be able to recognize a new lamp post. This is why when one moves through a network with multiple convolutional layers, data gets more abstract. Whereas the first convolutional layer will probably detect certain ‘large-scale patterns’ (like an object similar to a post exists yes/no, while trees may also be classified as ‘yes’); the second one will be able to look at the data at a more abstract level. For example, the filters of the layer will probably learn that a lamp post is pretty much vertical, and learn a vertical line detector. It will also learn that lamp posts have some kind of light structure, and thus that trees — with leaves — are probably not lamp posts. Making the data more abstract thus allows for better generalization and thus for better handling of new data! Traditional machine learning methods can often only handle data that is linear. But images are far from linear: suppose that we have a set of pictures with blue skies. In some of them, a ball flies through the sky, and our aim is to identify the pictures which contain a ball. If we would use a linear machine learning model, we would get into trouble. There is no way to define a linear function that can grasp the ball in full. Have you ever seen a (curved) line in the shape of a circle before? Yes, maybe the cosine and sine functions come to mind — but let’s stop here, because these are no lines. Deep neural networks use some kind of nonlinear activation functionto process more advanced, non-linear data. But convolutional layers do not provide this kind of non-linearity. What they do is to simply compute element-wise multiplications between a filter matrix and a matrix that contains a part of an image. We would thus need to add some non-linearity to our model. We use non-linearity layers for this, and we put them directly after the convolutional layer. Multiple non-linearity layer types exist, of which these are the most widely used: The preferred non-linear layer of choice these days is the ReLu layer, though. Researchers have identified that models using these type of activation functions (non-linear layers) are faster to train, which saves computational resources. This does not mean that sigmoid and tanh based CNNs are useless, possibly even the contrary. However, it will take longer to train, and for most tasks this would not be necessary, as ReLu would perform fine for them. Please note that a wide variety of activation functions are available these days. We will discuss this in a future post. It also includes activation functions that attempt to improve the ReLU activation function mentioned above. We know that the CNN’s convolutional layer reduces the input. This reduction is however very small: using one filter, the 32x32x3 RGB image input into the convolutional layer leads to a 28x28x1 output. But often, images are not 32x32 pixels: they are larger. This means that the output would still be very large. We saw before that in many cases, multiple convolutional layers are used in a CNN, for reasons of abstraction and generalization. The larger the input of a convolutional layer, the larger the convolutional operation (the sliding of the learnt filters over the images) it needs to perform, and the longer it will take to train the model. For the sake of computational resources, we would thus need to reduce the dimensions of the output of a convolutional layer. And here is where pooling layers come into view. Pooling layers will allow us to move over the output of the convolutional layer (and possibly, the ReLu layer) i.e. the activation map, and make it smaller. We move over the output often with a 2 x 2 structure and move again with the concept of stride, which is often 1. Multiple forms of pooling exist, but max pooling is the most widely used one. As images will say more than one thousand words, this perfectly demonstrates what max pooling does: (note that here, stride 2 was used). As you can see, if the activation map for the image was first (4,4,1), it will now be (2,2,1). This will ensure that future convolutional layers will have much more ease at training! With a real-world image example from a great source on CNNs: In this example, a convolutional layer produced a 224x224 activation map with 64 filters, which is downsampled to 112x112x64. This is four times smaller, saving a lot of computational resources! (the calculation: (112*112)/(224*224)). Suppose that we trained a CNN which needs to classify images as belonging to the class “lamp post” or the class “no lamp post”. Convolutional layers combined with non-linear layers and pooling layers allowed us to identify patterns in images, but they cannot predict the actual class. They’re simply not built to do that. But we have seen a typical structure before that is able to do precisely that. And it’s the fully-connected (a.k.a. densely-connected) neural network. It suits classification and regression tasks very well. Near the end of a CNN used for classification and regression, we will thus very likely see these kind of structures, packaged into a set of layers. For the same reason that they are used with convolutional layers, these layers will also have non-linear activation functions. The last layer will probably use the so-called softmax activation function since it will allow us to direct our output into a class. We also call this last layer the loss layer. Now that we covered the layers that are very often used in a CNN, we may talk about stacking them into an architecture. Since we’re talking about convolutional neural networks, the convolutional layers play a big role in these kind of architectures. This means that we will most probably start with a convolutional layer, which takes the image as input, and converts the input to an activation map with learnt filters. We will then use a ReLu layer to apply some non-linearity to the image. We then arrive at a crossroads. Do we need the data to be more abstract, so accuracy can improved? Especially for images this can be useful, but for smaller drawings this may not be necessarily true. If we need it to be more abstract, we will need to use additional convolutional layers. If we can move on to classification, we can use the fully connected layers with a softmax output layer. In both cases, we would need to apply downsampling (using, for example, max pooling) to reduce the dimensionality of the network. Consequently, we would start a CNN with: If we can move on to classification, we add the densely classified layers. If we need to make data more abstract, we would repeat the structure above until we can move on to classification. Repetition of these layers is perfectly normal and we encourage that once you build these kind of networks, you play around with layer stacking/repetition of layers, in order to find out how these kind of architectural changes impact the models. If you reached this point, you now know what a convolutional neural network is used for and what it is composed of. In a future blog, we will definitely cover the various architectures that are out there today. But for now, it’s time for you to rest, to let it sink in, and to have your knowledge about CNNs serve you as a basis for further development. Any comments, suggestions or questions are welcome as always. Thank you for reading!! -- -- BS in Computer Science | studying ML| amatuer philosopher| compulsive reader Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, Professor Francesco Franco explains how deep learning algorithms are suitable for computer vision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/@keith-mcnulty/how-do-mathematicians-prove-things-b5ad9060fe84\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkW8NFceYqUd",
        "outputId": "f0cf57f0-a3b8-4ded-8efe-379b0ff18ab1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Member-only story Keith McNulty Follow -- 5 Share The word ‘proof’ has different implications in different contexts. In the field of jurisprudence, for example, the phrase ‘proof beyond reasonable doubt’ is a common phrase used for the burden of proof. This suggests there is room for some doubt in a conviction, as long as it is not considered ‘reasonable’. Statisticians will often reject a hypothesis on the basis of a very low likelihood of the observed data sample occurring if the hypothesis were true. Again, there remains room for doubt in the conclusion, albeit very small. In the field of Pure Mathematics, however, the burden of proof is absolute. There can be no room for doubt whatsoever. Given a statement, the mathematician must be able to describe a logical path that leads to a conclusion that is 100% certain. 99.99999% is no good, because that remaining 0.00001% means that the statement is not completely proven. The logical path that the mathematician describes must depend only on definitions, things that are already known and proved true, or on a very small set of very basic axioms that are accepted to be self-evident by the mathematical community without… -- -- 5 Pure and Applied Mathematician. LinkedIn Top Voice in Tech. Expert and Author in Data Science and Statistics. Find me on LinkedIn, Twitter or keithmcnulty.org Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, mathematician Keith McNulty explains why there is no room for proof in a conviction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/@amosgyamfi/the-6-best-llm-tools-to-run-models-locally-eedd0f7c2bbd\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjaRDi1qY8jD",
        "outputId": "4a66bcdf-5e8a-4a1b-886a-6c673ca08cc2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Amos Gyamfi Follow -- 16 Listen Share Running large language models (LLMs) like ChatGPT and Claude usually involves sending data to servers managed by OpenAI and other AI model providers. While these services are secure, some businesses prefer to keep their data entirely offline for greater privacy. This article covers the top six tools developers can use to run and test LLMs locally, ensuring their data never leaves their devices, similar to how end-to-end encryption protects privacy. A tool like LM Studio does not collect user data or track users’ actions when they use it to run local LLMs. It lets all your chat data stay on your local machine without sharing with an AI/ML server. Depending on your specific use case, there are several offline LLM applications you can choose. Some of these tools are completely free for personal and commercial use. Others may require sending them a request for business use. There are several local LLM tools available for Mac, Windows, and Linux. The following are the six best tools you can pick from. LM Studio can run any model file with the format gguf. It supports gguf files from model providers such as Llama 3.1, Phi 3, Mistral, and Gemma. To use LM Studio, visit the link above and download the app for your machine. Once you launch LM Studio, the homepage presents top LLMs to download and test. There is also a search bar to filter and download specific models from different AI providers. Searching for a model from a specific company presents several models, ranging from small to large quantization. Depending on your machine, LM Studio uses a compatibility guess to highlight the model that will work on that machine or platform. LM Studio provides similar functionalities and features as ChatGPT. It has several functions. The following highlights the key features of LM Studio. The local server provides sample Curl and Python client requests. This feature helps to build an AI application using LM Studio to access a particular LLM. With the above sample Python code, you can reuse an existing OpenAI configuration and modify the base url to point to your localhost. This tool is free for personal use and it allows developers to run LLMs through an in-app chat UI and playground. It provides a gorgeous and easy to use interface with filters and supports connecting to OpenAI’s Python library without the need for an API key. Companies and businesses can use LM studio on request. However it requires a M1/M2/M3 Mac or higher, or a Windows PC with a processor that supports AVX2. Intel and AMD users are limited to using the Vulkan inference engine in v0.2.31. Think of Jan as an open-source version of ChatGPT designed to operate offline. It is built by a community of users with a user-owned philosophy. Jan allows you to run popular models like Mistral or Llama on your device without connecting it to the internet. With Jan, you can access remote APIs like OpenAI and Groq. Jan is an electron app with features similar to LM Studio. It makes AI open and accessible to all by turning consumer machines into AI computers. Since it is an open source project, developers can contribute to it and extend its functionalities. The following breaksdown the major features of Jan. Jan provides a clean and simple interface to interact with LLMs and it keeps all your data and processing information locally. It has over seventy large language models already installed for you to use. The availability of these ready-to-use models makes it easy to connect and interact with remote APIs like OpenAI and Mistral. Jan also has a great GitHub, Discord, and Hugging Face communities to follow and ask for help. However, like all the LLM tools, the models work faster on Apple Silicon Macs than on Intel ones. Llamafile is backed by Mozilla whose aim is to support and make open source AI accessible to everyone using a fast CPU inference with no network access. It converts LLMs into multi-platform Executable Linkable Format (ELF). It provides one of the best options to integrate AI into applications by allowing you to run LLMs with just a single executable file. It is designed to convert weights into several executable programs that require no installation to run on architectures such as Windows, MacOS, Linux, Intel, ARM, FreeBSD, and more. Under the hood, Llamafile uses tinyBLAST to run on OSs like Windows without requiring an SDK. llamafile-convert mistral-7b.gguf To install Llamafile, head to the Huggingface website, select Models from the navigation, and search for Llamafile. You can also install your preferred quantized version from the URL below. https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/tree/main Note: The larger the quantization number, the better the response. As highlighted in the image above, this article uses Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile where Q6 represents the quantization number. Step 1: Download Llamafile From the link above, click any of the download buttons to get your preferred version. If you have the wget utility installed on your machine, you can download Llamafile with the command below. wget https://huggingface.co/Mozilla/Meta-Llama-3.1-8B-Instruct-llamafile/blob/main/Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile You should replace the URL with the version you like. Step 2: Make Llamafile Executable After downloading a particular version of Llamafile, you should make it executable using the following command by navigating to the file’s location. chmod +x Meta-Llama-3.1-8B-Instruct.Q6_K.llamafileStep 3: Run Llamafile Prepend a period and forward slash ./ to the file name to launch Llamafile. ./Meta-Llama-3.1-8B-Instruct.Q6_K.llamafile The Llamafile app will now be available at http://127.0.0.1:8080 to run your various LLMs. Llamafile helps to democratize AI and ML by making LLMs easily reachable to consumer CPUs. As compared to other local LLM apps like Llama.cpp, Llamafile gives the fastest prompt processing experience and better performance on gaming computers. Since it has a faster performance, it is an excellent option for summarizing long text and large documents. It runs 100% offline and privately, so users do not share their data to any AI server or API. Machine Learning communities like Hugging Face supports the Llamafile format, making it easy to search for Llamafile related models. It also has a great open source community that develops and extends it further. GPT4ALL is built upon privacy, security, and no internet-required principles. Users can install it on Mac, Windows, and Ubuntu. Compared to Jan or LM Studio, GPT4ALL has more monthly downloads, GitHub Stars, and active users. GPT4All can run LLMs on major consumer hardware such as Mac M-Series chips, AMD and NVIDIA GPUs. The following are its key features. To start using GPT4All to run LLMs locally, Download the required version for your operating system. With the exception of Ollama, GPT4ALL has the most significant number of GitHub contributors and about 250000 monthly active users (according to https://www.nomic.ai/gpt4all) and compared to its competitors. The app collects anonymous user data about usage analytics and chat sharing. However, users have the options to opt in or out. Using GPT4ALL, developers benefit from its large user base, GitHub, and Discord communities. Using Ollama, you can easily create local chatbots without connecting to an API like OpenAI. Since everything runs locally, you do not need to pay for any subscription or API calls. To use Ollama for the first time, visit https://ollama.com and download the version for your machine. You can install it on Mac, Linux, or Windows. Once you install Ollama, you can check its detailed information in Terminal with the following command. ollama To run a particular LLM, you should download it with: ollama pull modelname, where modelname is the name of the model you want to install. Checkout Ollama on GitHub for some example models to download. The pull command is also used for updating a model. Once it is used, only the difference will be fetched. After downloading for example, llama3.1, running ollama run llama3.1 in the command line launches the model. In the above example, we prompt the llama3.1 model to solve a Physics work and energy question. Ollama has over 200 contributors on GitHub with active updates. It has the largest number of contributors and is more extendable among the other open-source LLM tools discussed above. LLaMa.cpp is the underlying backend technology (inference engine) that powers local LLM tools like Ollama and many others. Llama.cpp supports significant large language model inferences with minimal configuration and excellent local performance on various hardware. It can also run in the cloud. To run your first local large language model with llama.cpp, you should install it with: brew install llama.cpp Next, download the model you want to run from Hugging Face or any other source. For example, download the model below from Hugging Face and save it somewhere on your machine. https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3.Q4_K_M.gguf Using your preferred command line tool like Terminal, cd into the location of the .gguf model file you just downloaded and run the following commands. In summary, you first invoke the LLaMa CLI tool and set color and other flags. The -m flag specifies the path of the model you want to use. The -p flag specifies the prompt you wish to use to instruct the model. After running the above command, you will see the result in the following preview. Running LLMs locally can help developers who want to understand their performance and how they work in detail. Local LLMs can query private documents and technical papers so that information about these documents does not leave the devices used to query them to any cloud AI APIs. Local LLMs are useful in no-internet locations and places where network reception is poor. In a telehealth setting, local LLMs can sort patient documents without having to upload them to any AI API provider due to privacy concerns. Knowing the performance of a large language model before using it locally is essential for getting the required responses. There are several ways you can determine the performance of a particular LLM. Here are a few ways. To answer the above questions, you can check excellent resources like Hugging Face and Arxiv.org. Also, Open LLm Leaderboard and LMSYS Chatbot Arena provide detailed information and benchmarks for varieties of LLMs. As discussed in this article, several motives exist for choosing and using large language models locally. You can fine-tune a model to perform a specialized task in a telemedicine app if you do not wish to send your dataset over the internet to an AI API provider. Many open-source Graphic User Interface (GUI-based) local LLM tools like LLm Studio and Jan provide intuitive front-end UIs for configuring and experimenting with LLMs without subscription-based services like OpenAI or Claude. You also discovered the various powerful command-line LLM applications like Ollama and LLaMa.cpp that help you run and test models locally and without an internet connection. Check out Stream’s AI Chatbot solution to integrate an AI chat into your app and visit all the related links to learn more. Originally published at https://getstream.io. -- -- 16 iOS Developer Advocate @getstream.io | visionOS Content Creator Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, the BBC’s Amos Gyamfi looks at the best ways to run artificial intelligence (AI) applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to handle whitespace and newlines\n",
        "WHITESPACE_HANDLER = lambda k: re.sub(r'\\s+', ' ', re.sub(r'\\n+', ' ', k.strip()))\n",
        "\n",
        "# Fetch the article content from the URL\n",
        "def fetch_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract all text from the article (this part might need adjustments based on the structure of the webpage)\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "# URL of the article\n",
        "article_url = \"https://medium.com/towards-data-science/make-the-switch-from-software-engineer-to-ml-engineer-7a4948730c97\"\n",
        "\n",
        "# Get the article text\n",
        "article_text = fetch_article_text(article_url)\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input text\n",
        "input_ids = tokenizer(\n",
        "    [WHITESPACE_HANDLER(article_text)],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=1000\n",
        ")[\"input_ids\"]\n",
        "\n",
        "# Generate summary\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=84,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_beams=4\n",
        ")[0]\n",
        "\n",
        "# Decode and print summary\n",
        "summary = tokenizer.decode(\n",
        "    output_ids,\n",
        "    skip_special_tokens=True,\n",
        "    clean_up_tokenization_spaces=False\n",
        ")\n",
        "print(article_text,\"\\n\")\n",
        "print(\"Summary : \", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJt4Dh-kZfC6",
        "outputId": "9cdbecf7-bb76-41ef-8fa7-83df1bcc2fca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sign up Sign in Sign up Sign in Kartik Singhal Follow Towards Data Science -- 4 Listen Share I receive a lot of inquiries (a LOT) about how to transition from a software engineer to a machine learning engineer (MLE) at FAANG companies. Having successfully transitioned myself, I can say that the biggest challenge I faced was not knowing where to start and feeling lost without a clear plan. In this article, I am sharing the step-by-step approach that will help you navigate this change. These 7 steps helped me transition from a software engineer to Machine Learning engineer. Let’s dive in. Why Machine Learning? Machine Learning and AI are super hot right now, but you should understand why you want to get into it. This personal motivation will keep you going even when the AI hype dies down. What Got Me Hooked: For me, it was about how Google search was developed. The way Google could find exactly what I needed so quickly really made me want to know more about the tech behind it. That curiosity got me into Learning to Rank algorithms starting with PageRank and then broader machine learning. Questions to Ask Yourself: It took me 4 years (1 year in Masters, 1 year in PhD where I dropped out, and 2 years in the industry) to realize what I really wanted to do. This is ok. It takes time to build experience and know enough about a new field which is as big as ML. Understanding your motivations and interests will naturally lead you to identify where you can best apply your skills within the ML landscape. I started working as a software engineer in the Amazon Pricing team. Even though Pricing as a domain was not my preferred choice, but due to extensive amount of experience I acquired there, it helped me to transition to MLE much faster. In your career, you’ll sometimes face decisions that require short-term sacrifices for long-term gains, especially when entering a new field. Here are some tough choices I had to make during my switch: If you’ve pinned down a domain you’re passionate about, you’ll still need a supportive manager and company to make the transition successfully. Find the Right Environment: Tip: Find teams that has transitioned Software Engineers to MLEs in the past. This can greatly accelerate your transition as these teams often have a clear guideline for the transition. Tip: Always draft a document outlining your transition plan and the projects you’d like to work on and discuss in your 1:1s with your manager. If they repeatedly show disinterest, they might not be motivated to help you switch roles. In my first team at Amazon, I gave my 200% as a software engineer, even though the role wasn’t my ideal choice. My goal was to make myself indispensable, allowing me to choose the projects I wanted to work on. This effort built a trusting relationship with my manager, where we valued each other’s advice. Why is this important? Typically, only top engineers get to choose their projects, while others must tackle the tasks assigned to them. Demonstrating reliability can give you opportunities that might otherwise be unattainable and give you more control over your career path. Once you’ve joined a team with ML opportunities, a supportive manager, and relevant domain space, it’s time to apply your foundational knowledge. Work on small projects on the side: For instance, I worked on a project to improve the AWS SageMaker training pipeline in my team at Amazon. This allowed me to work more closely with ML engineers in the team, understand their development process and contribute to development of new features in upcoming model iterations. Expand Your Scope: Tip: Read blogs and research articles from other companies within the same space to understand challenges faced by companies to get potential ideas for improvement. For example when I was at Amazon, I followed tech articles from other eCommerce platforms like eBay and Walmart. Transitions like promotions are lagging indicators, meaning that any new role requires the individual to already be performing at the level expected for that role. Identify the criteria that will be used for evaluation during your transition to an MLE role. Generally, Software Engineers and MLEs are evaluated differently during performance feedback sessions. With Software Engineer, often the emphasis is more on scalable system design, code quality and project complexity. With MLE, generally the emphasis is much more on Impact to the business metric and technical expertise. This is because, ML has a longer cycle of development compared to software engineering and are often directly tied to specific business metrics. The Software Engineer to MLE transition can be as challenging as it is rewarding. It requires a blend of strategic planning, continuous learning, and adaptability. Few more bonus tips: These strategies have been instrumental in navigating my career transition effectively. By following above steps, you can improve your journey and set a solid foundation for success in your new role as a Machine Learning Engineer. Best of luck and as always Happy Learning! If this article was helpful to you and you want to learn more about real-world tips for Machine Learning, Sign up for my newsletter or connect with me on LinkedIn. Disclaimer: This blog is based on personal experiences and publicly available resources. Please note, the opinions expressed are solely my own and do not represent those of my past or current employers. Always refer to official resources and guidelines from hiring companies for the most accurate information. -- -- 4 Towards Data Science Help Status About Careers Press Blog Privacy Terms Text to speech Teams \n",
            "\n",
            "Summary :  In our series of letters from African journalists, Kartik Singhal explains how to transition from software engineer to machine learning engineer.\n"
          ]
        }
      ]
    }
  ]
}